{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Suppress unnecessary Shapely warning\n",
    "warnings.filterwarnings('ignore',\n",
    "                        '.*Shapely GEOS version.*')\n",
    "\n",
    "from aiohttp import ClientSession\n",
    "from requests import request, Session\n",
    "from itertools import product, repeat\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "from threading import Thread\n",
    "import time\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import shapely\n",
    "import pygeos\n",
    "import contextily as cx\n",
    "from functools import reduce\n",
    "from pandas.plotting import lag_plot\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import copy\n",
    "import math\n",
    "from decimal import Decimal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import matplotlib as mpl\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from multiprocess import Process, Pool\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# Set up Pandas defaults\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c2c96",
   "metadata": {},
   "source": [
    "### Define Script Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26658946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the beginning year for job data. If it is past June,\n",
    "# just analyze this year's job growth data from the beginning\n",
    "# of the year to the present. If it is\n",
    "# before June, analyze data from the beginning of last\n",
    "# year to the present. Note below we test to see if\n",
    "# it is after July, because if it is, we should have June's\n",
    "# data at that point.\n",
    "now = datetime.today()\n",
    "if now.month > 7:\n",
    "    begin_job_year = now.year\n",
    "else:\n",
    "    begin_job_year = now.year - 1\n",
    "\n",
    "# Get the most recent year of census data\n",
    "def get_end_year():\n",
    "    \"\"\"\n",
    "    The ACS typically releases the previous year's\n",
    "    data on Dec 15th. So we will determine the end_year\n",
    "    based on the current date. For example, if it's\n",
    "    August 8th, 2023, that is before December 15th, 2023.\n",
    "    So in this case, we will make the end year 2021, as\n",
    "    the 2022 data will likely be released by Dec 15th,\n",
    "    2023. Otherwise, if we are accessing this data between\n",
    "    Dec 15th and the end of the current year, we will take\n",
    "    the current year and subtract by 1, as the previous\n",
    "    year's data is likely released.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get today\n",
    "    now = datetime.today()\n",
    "\n",
    "    # get this year\n",
    "    this_year = now.year\n",
    "\n",
    "    # Make Dec 15th\n",
    "    dec_15 = datetime(this_year, 12, 15)\n",
    "\n",
    "    # If it's before Dec 15th of this year,\n",
    "    # the end_year will be the current year\n",
    "    # subtracted by 2 (see function docstring)\n",
    "    if now < dec_15:\n",
    "        end_year = this_year - 2\n",
    "    else:\n",
    "        end_year = this_year - 1\n",
    "\n",
    "    return end_year\n",
    "\n",
    "# Get the census end year\n",
    "census_end_year = get_end_year()\n",
    "\n",
    "# Census beginning year is end_year - 5\n",
    "census_begin_year = census_end_year - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244d05d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Read in all helper codes\n",
    "\n",
    "# Read in state FIPS codes\n",
    "state_fips = pd.read_csv(\n",
    "    \"datasets/helper_datasets/state_FIPS_codes.csv\",\n",
    "    dtype={'state_code':str}\n",
    ")\n",
    "\n",
    "# Read in MSA state codes\n",
    "msa_state_fips = pd.read_csv(\n",
    "    \"datasets/helper_datasets/msa_and_state_codes.csv\",\n",
    "    dtype={'FIPS State Code':str, 'CBSA Code':str}\n",
    ")\n",
    "\n",
    "# Get only necessary columns\n",
    "msa_state_fips = msa_state_fips[['CBSA Code','CBSA Title','FIPS State Code']]\n",
    "\n",
    "# Rename column\n",
    "msa_state_fips.rename(columns={'FIPS State Code':'state_code'}, inplace=True)\n",
    "\n",
    "# Here is where to read in the crosswalk file\n",
    "msa_city_crosswalk = pd.read_csv(\"datasets/helper_datasets/msa_to_city_crosswalk.csv\",\n",
    "                                 dtype={\"msa_geoid\":str, \"city_geoid\":str})\n",
    "msa_city_crosswalk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67419cf",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to create directory\n",
    "def create_folder(the_path):\n",
    "    if not os.path.isdir(the_path):\n",
    "        os.mkdir(the_path)\n",
    "        \n",
    "# Turn into datetime format\n",
    "def turn_df_into_datetime(\n",
    "    dataframe,\n",
    "    msa=False,\n",
    "    city=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Turns a dataframe created by the API functions\n",
    "    into a tidy datetime format.\n",
    "    \"\"\"\n",
    "    # Make a copy\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # Set index\n",
    "    if msa:\n",
    "        df = df.set_index(['msa_name','msa_code'])\n",
    "    elif city:\n",
    "        df = df.set_index(['name','geo_id'])\n",
    "    else:\n",
    "        raise ValueError(\"Please define MSA or City in arguments.\")\n",
    "    \n",
    "    # Stack\n",
    "    df = df.stack()\n",
    "    \n",
    "    # Turn into dataframe\n",
    "    df = pd.DataFrame(df).reset_index()\n",
    "    \n",
    "    # Rename the post-stacked columns\n",
    "    df.rename(columns={'level_2':'year', 0:'value'}, inplace=True)\n",
    "    \n",
    "    # Make year column integer\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    \n",
    "    # Make datetime column\n",
    "    df['date'] = pd.to_datetime(df['year'], format='%Y')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function for Census Datasets\n",
    "def prep_census_datasets(\n",
    "    dataframe, \n",
    "    msa=False,\n",
    "    city=False,\n",
    "    msa_state_fips=msa_state_fips,\n",
    "    msa_city_cross_walk=msa_city_crosswalk,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function preps the census datasets into\n",
    "    the same format as the BLS job datasets\n",
    "    for future city comparisons using both\n",
    "    Census and BLS datasets.\n",
    "    \"\"\"\n",
    "    # Make copy\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # Turn into tidy datetime\n",
    "    df = turn_df_into_datetime(\n",
    "        df, msa=msa, city=city)\n",
    "\n",
    "    # Add MSA state code\n",
    "    if msa:\n",
    "        df = df.merge(msa_state_fips, \n",
    "                        how='left', \n",
    "                        left_on=['msa_name','msa_code'],\n",
    "                        right_on=['CBSA Title','CBSA Code'])\n",
    "        \n",
    "        # Drop unnecessary columns\n",
    "        df.drop(columns=['CBSA Title','CBSA Code'], inplace=True)\n",
    "\n",
    "        # Replace NECTA Division\n",
    "        df['msa_name'] = df['msa_name'].apply(lambda x: x.replace(\" NECTA Division\",\"\"))\n",
    "        df['msa_name'] = df['msa_name'].apply(lambda x: x.replace(\" NECTA\",\"\"))\n",
    "        \n",
    "    elif city:\n",
    "        df = df.merge(msa_city_cross_walk,\n",
    "                      how='left',\n",
    "                      left_on=['geo_id'],\n",
    "                      right_on=['city_geoid'])\n",
    "        \n",
    "        # Create state name column\n",
    "        df['state'] = df['name'].apply(lambda x: str(x.split(\", \")[-1]))\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        df.drop(columns=['name','geo_id'], inplace=True)\n",
    "        \n",
    "        # Drop city geo_ids that are not part of any MSA\n",
    "        df = df[df['city_geoid'].notna()].reset_index(drop=True)\n",
    "        \n",
    "        # Remake city name column\n",
    "        df['city_name'] = df['city_name'] + \", \" + df['state']\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Please specify MSA or City in the arguments.\")\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define normalizing function\n",
    "def normalize_column(\n",
    "    series, mean_standardize=False, \n",
    "    min_max_standardized=False):\n",
    "    \"\"\"\n",
    "    Normalizes a column's values.\n",
    "    \n",
    "    Arguments\n",
    "    -----------\n",
    "        series (Series): A pandas Series, which can\n",
    "            simply be passed as a column of a\n",
    "            DataFrame.\n",
    "            \n",
    "    Returns\n",
    "    -----------\n",
    "        series (Series): A normalized Series, which can\n",
    "            be set to a column in a DataFrame.\n",
    "    \"\"\"\n",
    "    # Make a copy\n",
    "    sr = series.copy()\n",
    "    \n",
    "    # Standardize around the mean or by min-max\n",
    "    if mean_standardize:\n",
    "        # Make normalized column\n",
    "        sr = (sr - sr.mean())/sr.std()\n",
    "    elif min_max_standardized:\n",
    "        # Make normalized column\n",
    "        sr = (sr - sr.min())/(sr.max()-sr.min())\n",
    "    else:\n",
    "        raise ValueError(\"Please specify how to normalize.\")\n",
    "    \n",
    "    return sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30dde6",
   "metadata": {},
   "source": [
    "## Read in City Datasets\n",
    "\n",
    "1. Population (B01003_001E)\n",
    "2. Median Income\n",
    "3. Median Unit Price\n",
    "4. Median Rent\n",
    "5. Total Units\n",
    "6. Percent Renter Occupied\n",
    "7. Total Employed (B23025_004E)\n",
    "8. Rent-to-Price Ratio\n",
    "9. People-per-Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c98261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of demographics to iterate through\n",
    "demo_list = [\n",
    "    'population',\n",
    "    'median_income',\n",
    "    'median_price',\n",
    "    'median_rent',\n",
    "    'people_per_unit',\n",
    "    'percent_renter_occupied',\n",
    "    'population',\n",
    "    'rent_price_ratio',\n",
    "    'total_employed',\n",
    "]\n",
    "\n",
    "# Create dictionary to save dataframes to\n",
    "demo_dict = {}\n",
    "\n",
    "# Loop through demos and read in dataframes\n",
    "for demo in demo_list:\n",
    "    \n",
    "    # Read in dataframe\n",
    "    df = pd.read_csv(\n",
    "        f\"datasets/cleaned_census_api_files/city_data/{demo}_city.csv\",\n",
    "        dtype={'geo_id':str}\n",
    "    )\n",
    "\n",
    "    # Run prep function to get into correct format\n",
    "    df = prep_census_datasets(\n",
    "        df, city=True)\n",
    "\n",
    "    # Add dataframe to dictionary\n",
    "    demo_dict[demo] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf39d3",
   "metadata": {},
   "source": [
    "#### Read in Job data at the MSA level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16878a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in most recent job data\n",
    "jobs = pd.read_csv('datasets/bls/raw/most_recent_bls_data.csv',\n",
    "                   dtype={'msa_code':str, 'state_code':str})\n",
    "\n",
    "# Make sure the date column is in datetime format\n",
    "jobs['date'] = pd.to_datetime(jobs['date'])\n",
    "\n",
    "# Replace NECTA Division\n",
    "jobs['msa_name'] = jobs['msa_name'].apply(lambda x: x.replace(\" NECTA Division\",\"\"))\n",
    "jobs['msa_name'] = jobs['msa_name'].apply(lambda x: x.replace(\" NECTA\",\"\"))\n",
    "\n",
    "# Call in and merge crosswalk\n",
    "crosswalk = pd.read_csv(\"datasets/helper_datasets/msa_to_city_crosswalk.csv\",\n",
    "                       dtype={'msa_geoid':str, 'city_geoid':str})\n",
    "\n",
    "# Only keep IDs\n",
    "crosswalk = crosswalk[['msa_geoid','city_geoid']]\n",
    "\n",
    "# Rename jobs' msa column\n",
    "jobs.rename(columns={'msa_code':'msa_geoid'}, inplace=True)\n",
    "\n",
    "# Merge crosswalk to job data\n",
    "jobs = jobs.merge(crosswalk, \n",
    "                          how='left', \n",
    "                          on='msa_geoid')\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cd26e",
   "metadata": {},
   "source": [
    "### Define Graphing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec14d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function that runs linear regression\n",
    "def run_lr(df, column):\n",
    "    \"\"\"\n",
    "    Run linear regression on time-series data \n",
    "    and return the coefficient and intercept.\n",
    "    \n",
    "    Arguments\n",
    "    -----------\n",
    "        df (DataFrame): A dataframe that contains the\n",
    "            target column and an 'ordinal_date' column\n",
    "            that was created by a time-series column in \n",
    "            the format of \"%Y-%m-%d\" and making it ordinal,\n",
    "            such as running the code below in some other \n",
    "            step. \n",
    "            \n",
    "            EXAMPLE...\n",
    "            # Create ordinal column\n",
    "            df['ordinal_date'] = df['date'].map(\n",
    "                datetime.toordinal)\n",
    "                \n",
    "        column (str): The name of the target column.\n",
    "            \n",
    "    Returns\n",
    "    -----------\n",
    "        coef (float): The coefficient of the linear\n",
    "            equation calculated.\n",
    "        \n",
    "        intercept (float): The y-intercept of the linear\n",
    "            equation calculated.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Run linear regression\n",
    "    normal_lr = LinearRegression()\n",
    "    X = df[['ordinal_date']]\n",
    "    y = df[column]\n",
    "    normal_lr.fit(X, y)\n",
    "    coef = normal_lr.coef_[0]\n",
    "    intercept = normal_lr.intercept_\n",
    "\n",
    "    # Return lr coefficient\n",
    "    return coef, intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77205b2e",
   "metadata": {},
   "source": [
    "### Define Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e4627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all cities, sort by coefficient, plot top 10\n",
    "def plot_top_10_cities(\n",
    "    ranked_cities,\n",
    "    plotting_msa=False,\n",
    "    plotting_city=False,\n",
    "    plot_jobs=False,\n",
    "    plot_rent=False,\n",
    "    plot_income=False,\n",
    "    plot_price=False,\n",
    "    plot_units=False,\n",
    "    plot_rent_to_price=False,\n",
    "    plot_jobs_per_unit=False,\n",
    "    begin_year_1=2013,\n",
    "    plot_all=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the top cities for a given demographic. Top cities\n",
    "    are chosen based on their trend. This function can also \n",
    "    find the top cities based on multiple datasets.\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "        ranked_cities (DataFrame): A dataframe of cities\n",
    "            already ranked by various demographics. The\n",
    "            dataframe returned by the \"make_ranking()\"\n",
    "            function is the ideal dataframe to pass\n",
    "            to this function.\n",
    "            \n",
    "        plot_jobs (True/False): If Ture, plot jobs. Only one \n",
    "            demographic can be plotted at a time, so if you'd \n",
    "            like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "        \n",
    "        plot_rent (True/False): If Ture, plot rent. Only one \n",
    "            demographic can be plotted at a time, so if you'd \n",
    "            like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "        \n",
    "        plot_income (True/False): If Ture, plot income. Only one \n",
    "            demographic can be plotted at a time, so if you'd \n",
    "            like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "        \n",
    "        plot_price (True/False): If Ture, plot price. Only one \n",
    "            demographic can be plotted at a time, so if you'd \n",
    "            like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "        \n",
    "        plot_units (True/False): If Ture, plot units. Only one \n",
    "            demographic can be plotted at a time, so if you'd \n",
    "            like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "        \n",
    "        plot_rent_to_price (True/False): If Ture, plot rent-ro-price\n",
    "            ratio. Only one demographic can be plotted at a time, \n",
    "            so if you'd like to plot a different demographic, \n",
    "            this must be set to False.\n",
    "        \n",
    "        plot_jobs_per_unit (True/False): If Ture, plot jobs-per-unit. \n",
    "            Only one demographic can be plotted at a time, so if \n",
    "            you'd like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "            \n",
    "        begin_year_1 (int): The year you'd like the\n",
    "            analysis to start.\n",
    "            \n",
    "        plot_all (True/False): True if you want to plot every\n",
    "            city in the dataframe (slow). False if you only want\n",
    "            to plot the top 5 (fast).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "        coef_df (DataFrame): A dataframe with the rankings\n",
    "            of each city, from \"best to worst.\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Make a copy of the ranked cities\n",
    "    ranked = ranked_cities.copy()\n",
    "    \n",
    "    # Get folder name variable\n",
    "    if plotting_msa:\n",
    "        file_geo = 'msa'\n",
    "    elif plotting_city:\n",
    "        file_geo = 'city'\n",
    "    else:\n",
    "        raise ValueError(\"Please define MSA or City as arguments.\")\n",
    "    \n",
    "    # If not plotting all cities (and just top 10),\n",
    "    # keep only the top 10 cities in the dataframe\n",
    "    if not plot_all:\n",
    "        ranked = ranked.head(10)\n",
    "    \n",
    "    ### Call in the dataset we will be graphing from\n",
    "    \n",
    "    # If plotting job growth\n",
    "    if plot_jobs:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Job\"\n",
    "        \n",
    "        # Read in most recent job data\n",
    "        dataframe_1 = pd.read_csv('datasets/bls/raw/most_recent_bls_data.csv',\n",
    "                           dtype={'msa_code':str, 'state_code':str})\n",
    "\n",
    "        # Make sure the date column is in datetime format\n",
    "        dataframe_1['date'] = pd.to_datetime(dataframe_1['date'])\n",
    "\n",
    "        # Replace NECTA Division\n",
    "        dataframe_1['msa_name'] = dataframe_1['msa_name'].apply(lambda x: x.replace(\" NECTA Division\",\"\"))\n",
    "        dataframe_1['msa_name'] = dataframe_1['msa_name'].apply(lambda x: x.replace(\" NECTA\",\"\"))\n",
    "    \n",
    "    # If plotting rent growth\n",
    "    elif plot_rent:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Median Rent\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            f\"datasets/cleaned_census_api_files/{file_geo}_data/median_rent_{file_geo}.csv\",\n",
    "            dtype={'msa_code':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, msa=msa, city=city)\n",
    "        \n",
    "    # If plotting income growth\n",
    "    elif plot_income:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Median Income\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            f\"datasets/cleaned_census_api_files/{file_geo}_data/median_income_{file_geo}.csv\",\n",
    "            dtype={'msa_code':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, msa=msa, city=city)\n",
    "        \n",
    "    # If plotting price growth\n",
    "    elif plot_price:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Median Price\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            f\"datasets/cleaned_census_api_files/{file_geo}_data/median_price_{file_geo}.csv\",\n",
    "            dtype={'msa_code':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, msa=msa, city=city)\n",
    "        \n",
    "    # If plotting unit growth\n",
    "    elif plot_units:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Total Units\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            f\"datasets/cleaned_census_api_files/{file_geo}_data/total_units_{file_geo}.csv\",\n",
    "            dtype={'msa_code':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, msa=msa, city=city)\n",
    "        \n",
    "    # If plotting rent-to-price\n",
    "    elif plot_rent_to_price:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Rent-to-Price\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            f\"datasets/cleaned_census_api_files/{file_geo}_data/rent_price_ratio_{file_geo}.csv\",\n",
    "            dtype={'msa_code':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, msa=msa, city=city)\n",
    "        \n",
    "    # If plotting jobs-per-unit\n",
    "    elif plot_jobs_per_unit:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Jobs per Unit\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            f\"datasets/cleaned_census_api_files/{file_geo}_data/jobs_per_unit_{file_geo}.csv\",\n",
    "            dtype={'msa_code':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, msa=msa, city=city)\n",
    "        \n",
    "    # Otherwise, print error statement\n",
    "    else:\n",
    "        print(\"Please specify a demographic to plot by setting it to True, and leaving the others set to False.\")\n",
    "        raise Exception(\"SPECIFY A DEMOGRAPHIC TO PLOT.\")\n",
    "    \n",
    "    # Make copy\n",
    "    main_df = dataframe_1.copy()\n",
    "            \n",
    "    # Create main variable to use for the rest of the script\n",
    "    column = 'value'\n",
    "\n",
    "    # Create dictionary to store filtered dataframes\n",
    "    filtered_dict = {}\n",
    "\n",
    "    # Set y_lim list to find max and min\n",
    "    y_lim_list_trend = []\n",
    "    y_min_list_trend = [0]\n",
    "    y_lim_list_pct = []\n",
    "    y_min_list_pct = [0]\n",
    "    \n",
    "    # Loop through all cities in the ranked dataframe\n",
    "    for city in ranked['msa_name'].dropna().unique():\n",
    "\n",
    "        # Isolate just that city\n",
    "        df = main_df[main_df['msa_name']==city].copy()\n",
    "        \n",
    "        # Sort by date\n",
    "        df = df.sort_values('date')\n",
    "                \n",
    "        # Create difference column\n",
    "        df['value_change'] = df['value'].diff()\n",
    "        \n",
    "        # Create pct_change column\n",
    "        df['percent_change'] = df['value'].pct_change()\n",
    "        \n",
    "        # Filter by beginning year\n",
    "        df = df[df['year']>=begin_year_1].reset_index(drop=True)\n",
    "                \n",
    "        # If an MSA's most recent year is after the beginning\n",
    "        # year, remove it from the graphs. For example, if we want to\n",
    "        # view the growth of all cities since 2016, but Prescott Valley\n",
    "        # only has data starting at 2019, this may skew the data.\n",
    "        if df['year'].iloc[0] != begin_year_1:\n",
    "            print(f\"Dropping {city}, it's dataframe has a smaller window.\")\n",
    "            continue\n",
    "                \n",
    "        # Remove NaN values\n",
    "        df = df[df['percent_change'].notna()]\n",
    "        \n",
    "        # Isolate date and value columns\n",
    "        df = df[[\n",
    "            'date', 'value', 'value_change',\n",
    "            'percent_change']].reset_index(drop=True)\n",
    "        \n",
    "        # Add this dataframe's y_lim to list\n",
    "        y_lim_list_trend.append(df['value'].max())\n",
    "        y_min_list_trend.append(df['value'].min())\n",
    "        y_lim_list_pct.append(df['percent_change'].max())\n",
    "        y_min_list_pct.append(df['percent_change'].min())\n",
    "        \n",
    "        # get next months's datetime\n",
    "        next_year = df['date'].iloc[-1] + relativedelta(months=1)\n",
    "        \n",
    "        # Create ordinal column\n",
    "        df['ordinal_date'] = df['date'].map(datetime.toordinal)\n",
    "        \n",
    "        # Run linear regression and get the trend's coefficient\n",
    "        coef_value, intercept_value = run_lr(df, column='value')\n",
    "        coef_pct, intercept_pct = run_lr(df, column='percent_change')\n",
    "        \n",
    "        # Create next year's date\n",
    "        df.loc[len(df.index)] = [\n",
    "            next_year, np.nan, np.nan, \n",
    "            np.nan, datetime.toordinal(next_year)]\n",
    "        \n",
    "        # Create averages column\n",
    "        the_average_pct = df['percent_change'].mean()\n",
    "        df['average_pct'] = the_average_pct\n",
    "        the_average_value = df['value_change'].mean()\n",
    "        df['average_value'] = the_average_value\n",
    "\n",
    "        # Fill in with linear regression values.\n",
    "        # Also add highest trend value to lim_list.\n",
    "        df['value_trend'] = df['ordinal_date']*coef_value + intercept_value\n",
    "        df['percent_change_trend'] = df['ordinal_date']*coef_pct + intercept_pct\n",
    "\n",
    "        # Also add highest trend value to lim_list\n",
    "        y_lim_list_trend.append(df['value_trend'].max())\n",
    "        y_lim_list_pct.append(df['percent_change_trend'].max())\n",
    "\n",
    "        # Get the y_lim\n",
    "        y_lim_trend = max(y_lim_list_trend) * 1.1\n",
    "        y_min_trend = min(y_min_list_trend)\n",
    "        y_lim_pct = max(y_lim_list_pct) * 1.1\n",
    "        y_min_pct = min(y_min_list_pct)\n",
    "            \n",
    "        # Save filtered data to dictionary\n",
    "        filtered_dict[city] = df\n",
    "    \n",
    "    # Loop through each city in the ranked df\n",
    "    for city_name in ranked['msa_name']:\n",
    "\n",
    "        # Get the job data\n",
    "        df = filtered_dict[city_name]    \n",
    "            \n",
    "        # Make a grid to plot 2 graphs on\n",
    "        fig = plt.figure(figsize=(12,3), dpi=300)\n",
    "        gs = GridSpec(nrows=1, ncols=2)\n",
    "        ax1 = fig.add_subplot(gs[0,0])\n",
    "        ax2 = fig.add_subplot(gs[0,1])\n",
    "        ax_list = [ax1, ax2]\n",
    "        \n",
    "        # Set grid\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "        # Set title\n",
    "        fig.suptitle(f\"{city_name}\\n\\n\\n\", \n",
    "             fontweight=\"bold\")\n",
    "\n",
    "        # Plot first graph\n",
    "        ax1 = df.plot(x='date',y='value', ax=ax1)\n",
    "        ax1 = df.plot(x='date',y='value_trend', ax=ax1, linestyle=\"--\")\n",
    "\n",
    "        # Plot second graph\n",
    "        ax2 = df.plot(x='date',y='percent_change', ax=ax2)\n",
    "        ax2 = df.plot(x='date',y='percent_change_trend', ax=ax2, linestyle=\"--\")\n",
    "\n",
    "        # Second graph's zero line\n",
    "        df['zero'] = 0\n",
    "        ax2 = df.plot(x='date', y='zero', ax=ax2, color=\"grey\")\n",
    "\n",
    "        # Also plot the average line\n",
    "        ax2 = df.plot(x='date', y='average_pct', \n",
    "                      ax=ax2, color=\"black\", linestyle=\"-\")\n",
    "\n",
    "        # Set title's for both graphs\n",
    "        ax1.set_title(f\"{demographic_1} Growth\")\n",
    "        ax2.set_title(f\"Percent Change in {demographic_1}\")\n",
    "\n",
    "        # Set y lims and y ticks\n",
    "        ax1.set_ylim([y_min_trend, y_lim_trend])\n",
    "        ax2.set_ylim([y_min_pct, y_lim_pct])\n",
    "\n",
    "        # Set y limits\n",
    "        y_tick_list_trend = [\n",
    "            y_lim_trend*0.25, y_lim_trend*0.5, \n",
    "            y_lim_trend*0.75, y_lim_trend]\n",
    "        y_tick_list_pct = [\n",
    "            y_min_pct, y_min_pct*0.5, 0, \n",
    "            y_lim_pct*0.5, y_lim_pct]\n",
    "\n",
    "        # Set y_ticks\n",
    "        ax1.yaxis.set_major_locator(\n",
    "            mticker.FixedLocator(y_tick_list_trend))\n",
    "        ax2.yaxis.set_major_locator(\n",
    "            mticker.FixedLocator(y_tick_list_pct))\n",
    "\n",
    "        # Set y-tick labels\n",
    "        ax1.set_yticklabels(\n",
    "            ['{:,}'.format(round(float(x), 3)) for x in y_tick_list_trend])\n",
    "        ax2.set_yticklabels(\n",
    "            ['{:,}'.format(round(float(x), 3)) for x in y_tick_list_pct])\n",
    "        \n",
    "        # Give suptitle more room\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "\n",
    "        # Create folder to save graphs into\n",
    "        create_folder(\"graphs\")\n",
    "        create_folder(\"graphs/msa_graphs\")\n",
    "        create_folder(f\"graphs/msa_graphs/{city_name}\")\n",
    "\n",
    "        # Create filepath to save graph to\n",
    "        save_filepath = f\"graphs/msa_graphs/{city_name}/{demographic_1} Growth.png\"\n",
    "\n",
    "        # Save the graphs\n",
    "        plt.savefig(save_filepath)\n",
    "        \n",
    "        # Show plot\n",
    "        if not plot_all:\n",
    "            plt.show()\n",
    "        \n",
    "        # Clear plot\n",
    "        plt.close(\"all\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06117323",
   "metadata": {},
   "source": [
    "### Define Ranking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5cb8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE FUNCTION THAT MAKES A TOTAL RANK \n",
    "### BASED ON MULTIPLE DEMOGRAPHICS\n",
    "\n",
    "def make_city_trend_dataframes(\n",
    "    df_dict,\n",
    "    rank_msa=False,\n",
    "    rank_city=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function ranks the invest-ability of every\n",
    "    city based on the demographics passed. It\n",
    "    analyzes the total average growth per year, as \n",
    "    well as the relative average growth per year\n",
    "    (measured as the average percent growth per year).\n",
    "    \n",
    "    Arguments\n",
    "    -----------\n",
    "        df_dict (dict): A dictionary to be used if you\n",
    "            want to combine multiple dataframes for analysis\n",
    "            and plotting. If using this, the key should be\n",
    "            a string with the demographic name, and the value\n",
    "            should be a list containing the dataframe in position\n",
    "            0, and the beginning year in position 1. See below\n",
    "            for two examples...\n",
    "            \n",
    "            Example 1, One Extra Dataframe\n",
    "            {\"Median Rent\": [median_rent_df, 2013]}\n",
    "            \n",
    "            Example 2, Multiple Extra Dataframes\n",
    "            {\"Median Rent\": [median_rent_df, 2013],\n",
    "            \"Population\" : [population_df, 2013]}\n",
    "            \n",
    "        max_price (int): If you only want to measure and\n",
    "            compare MSAs up to a certain median price,\n",
    "            enter the max median price as an integer.\n",
    "            \n",
    "        min_rent_to_price (float): If you only want to measure and\n",
    "            compare MSAs up to a certain rent-to-price ratio\n",
    "            (based on median rent and median price values),\n",
    "            enter the minimum rent-to-price ratio as a float.\n",
    "            \n",
    "        use_total_trend (True/False): Set to True if you'd like\n",
    "            to include the total trend weights in the ranking\n",
    "            of MSAs. Use False if not.\n",
    "        \n",
    "        use_average_percent (True/False): Set to True if you'd like\n",
    "            to include the average percent weights in the ranking\n",
    "            of MSAs. Use False if not.\n",
    "        \n",
    "        total_trend_weight_dict (dict): A dictionary to set the\n",
    "            weights of each demo. For example, if you'd like to\n",
    "            multiply the \"Median Rent\" weights by 2, giving a bigger\n",
    "            weight to the \"Median Rent\" demographic, all you need\n",
    "            is to make the key \"Median Rent\" set to a value of 2.\n",
    "            This dictionary is specifically for total trend weights.\n",
    "            See the example below.\n",
    "            \n",
    "            EXAMPLE...\n",
    "            total_trend_weight_dict={\n",
    "                \"Jobs\":1,\n",
    "                \"Median Rent\":1}\n",
    "        \n",
    "        average_percent_weight_dict (dict): A dictionary to set the\n",
    "            weights of each demo. For example, if you'd like to\n",
    "            multiply the \"Median Rent\" weights by 2, giving a bigger\n",
    "            weight to the \"Median Rent\" demographic, all you need\n",
    "            is to make the key \"Median Rent\" set to a value of 2.\n",
    "            This dictionary is specifically for average percent weights.\n",
    "            See the example below.\n",
    "            \n",
    "            EXAMPLE...\n",
    "            average_percent_weight_dict={\n",
    "                \"Jobs\":3,\n",
    "                \"Median Rent\":3}\n",
    "                \n",
    "        plot_graphs (True/False): If True, ask for user inputs\n",
    "            and run the plot_top_10_cities() function.\n",
    "            \n",
    "    Returns\n",
    "    -----------\n",
    "        final_df (DataFrame): A dataframe with each city\n",
    "            sorted by total rank.\n",
    "    \"\"\"\n",
    "    # Make a list to add each dataframe to\n",
    "    df_list = []\n",
    "    \n",
    "    # Define geo_name\n",
    "    if rank_msa:\n",
    "        geo_name = 'msa_name'\n",
    "        geo_file = 'msa'\n",
    "        geo_id = 'msa_geoid'\n",
    "    elif rank_city:\n",
    "        geo_name = 'city_name'\n",
    "        geo_file = 'city'\n",
    "        geo_id = 'city_geoid'\n",
    "    else:\n",
    "        raise ValueError(\"Please define MSA or City in the arguments.\")\n",
    "    \n",
    "    # Rename all columns by appending the demo name,\n",
    "    # except for the MSA name and date, which we will \n",
    "    # use as the key to merge all dataframes.\n",
    "    for demo in df_dict:\n",
    "        \n",
    "        if demo != \"Jobs\":\n",
    "        \n",
    "            # Get dataframe\n",
    "            df = df_dict[demo][0].copy()\n",
    "\n",
    "            # Rename select columns\n",
    "            df.rename(columns={'value':f'value_{demo}',\n",
    "                              'year':f'year_{demo}'}, inplace=True)\n",
    "\n",
    "            # Drop state\n",
    "            if 'state' in df.columns:\n",
    "                df.drop(columns=['state'], inplace=True)\n",
    "                \n",
    "            # Add dataframe to list\n",
    "            df_list.append(df)\n",
    "\n",
    "                \n",
    "    # Define list of columns to merge on\n",
    "    merge_list = ['msa_name','date','msa_geoid','city_geoid','city_name']\n",
    "                \n",
    "    # Merge all dataframes\n",
    "    merged_df = reduce(lambda left, right: \n",
    "                       pd.merge(left, right, \n",
    "                                left_on=merge_list, \n",
    "                                right_on=merge_list,\n",
    "                                suffixes=(None, \"_y\"),\n",
    "                                how=\"outer\"), df_list)\n",
    "        \n",
    "    # Now merge Jobs into the dataframe\n",
    "    if \"Jobs\" in df_dict:\n",
    "        df = df_dict[\"Jobs\"][0].copy()\n",
    "        \n",
    "        # Rename select columns\n",
    "        df.rename(columns={'value':f'value_Jobs',\n",
    "                          'year':f'year_Jobs'}, inplace=True)\n",
    "        \n",
    "        # Clean Jobs\n",
    "        df = df[['msa_name','date','msa_geoid','city_geoid',\n",
    "                 'year_Jobs','value_Jobs']]\n",
    "        \n",
    "        # Make another merged list\n",
    "        merge_list = ['msa_name','date','msa_geoid','city_geoid']\n",
    "        \n",
    "        # Merge jobs dataframe\n",
    "        merged_df = merged_df.merge(df, \n",
    "                                    how='outer',\n",
    "                                    left_on=merge_list,\n",
    "                                    right_on=merge_list\n",
    "                                   )\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "#     display(merged_df[\n",
    "#         (merged_df['city_geoid']=='0100820')\n",
    "#         & (merged_df['date']>'2020-12-01')\n",
    "#     ])\n",
    "    \n",
    "        \n",
    "    # Loop through columns and clean out the rest\n",
    "    for col in merged_df.columns:\n",
    "        if ('month_' in col) | ('series_id_' in col):\n",
    "            merged_df.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # Create new df to store coefficients\n",
    "    coef_df = pd.DataFrame(\n",
    "        data=None, columns=['city_name', 'city_geoid','msa_name','msa_geoid'])\n",
    "    \n",
    "    # Add columns for every demo\n",
    "    for demo in df_dict:\n",
    "        coef_df[f'trend_coef_{demo}'] = None\n",
    "        coef_df[f'average_value_{demo}'] = None\n",
    "        coef_df[f'average_pct_{demo}'] = None\n",
    "        \n",
    "    # Make temporary coef_df to use later\n",
    "    temp_coef_1 = coef_df.copy()\n",
    "        \n",
    "    # Create ordinal column\n",
    "    merged_df['ordinal_date'] = merged_df['date'].map(datetime.toordinal)\n",
    "    \n",
    "    # Loop through all cities\n",
    "    for city_geoid in merged_df[geo_id].dropna().unique():\n",
    "        \n",
    "        # Isolate just that city\n",
    "        df = merged_df[merged_df[geo_id]==city_geoid].copy()\n",
    "        \n",
    "        # Get geo_id\n",
    "        the_geoid = df['city_geoid'].iloc[0]\n",
    "        \n",
    "        # Get city name\n",
    "        city = df['city_name'].iloc[0]\n",
    "        \n",
    "        # Get MSA geoid\n",
    "        msa_geoid = df['msa_geoid'].iloc[0]\n",
    "        \n",
    "        # Get MSA name\n",
    "        msa_name = df['msa_name'].iloc[0]\n",
    "        \n",
    "        \n",
    "        # If there are population numbers less than 10,000, just\n",
    "        # don't include the city\n",
    "        if df['value_Population'].min() < 10_000:\n",
    "            continue\n",
    "        \n",
    "        # Sort by date\n",
    "        df = df.sort_values('date')\n",
    "        \n",
    "        # Make duplicate\n",
    "        temp_coef_2 = temp_coef_1.copy()\n",
    "        \n",
    "        # Set temp coef_df\n",
    "        temp_coef_2.loc[len(temp_coef_2.index)] = np.nan\n",
    "        temp_coef_2['city_name'] = city\n",
    "        temp_coef_2['city_geoid'] = the_geoid\n",
    "        temp_coef_2['msa_name'] = msa_name\n",
    "        temp_coef_2['msa_geoid'] = msa_geoid\n",
    "        \n",
    "        \n",
    "        # Loop through each demo\n",
    "        for demo in df_dict:\n",
    "                        \n",
    "            # Test to see if there's data for the demo\n",
    "            if df[df[f'year_{demo}'].notna()].shape[0] > 0:\n",
    "                                \n",
    "                # Make copy\n",
    "                df_temp = df.copy()\n",
    "                \n",
    "                # Get beginning year\n",
    "                begin_year = df_dict[demo][1]\n",
    "                                    \n",
    "                # Filter by beginning year minus 1\n",
    "                df_temp = df[df[f'year_{demo}']>=begin_year-1].reset_index(drop=True)\n",
    "                        \n",
    "                # Create difference column\n",
    "                df_temp[f'value_change_{demo}'] = df_temp[f'value_{demo}'].diff()\n",
    "\n",
    "                # Create pct_change column\n",
    "                df_temp[f'percent_change_{demo}'] = df_temp[f'value_{demo}'].pct_change()\n",
    "                                \n",
    "                # Filter by beginning year\n",
    "                df_temp = df_temp[\n",
    "                    df_temp[f'year_{demo}']>=begin_year].reset_index(drop=True)\n",
    "                \n",
    "                # If an MSA's most recent year is after the beginning\n",
    "                # year, remove it from the graphs. For example, if we want to\n",
    "                # view the growth of all cities since 2016, but Prescott Valley\n",
    "                # only has data starting at 2019, this may skew the data.\n",
    "                if df_temp[f'year_{demo}'].iloc[0] != begin_year:\n",
    "                    continue\n",
    "\n",
    "                # Remove NaN values\n",
    "                df_temp = df_temp[df_temp[f'percent_change_{demo}'].notna()]\n",
    "                \n",
    "                # If there are no non-NaN percent-change values,\n",
    "                # remove it from the graphs. This can happen due to\n",
    "                # every value being 0 for every year.\n",
    "                if df_temp.shape[0] == 0:\n",
    "                    continue\n",
    "                    \n",
    "\n",
    "                                    \n",
    "                # Run linear regression\n",
    "                coef_value, intercept_value = run_lr(df_temp, column=f'value_{demo}')\n",
    "                coef_pct, intercept_pct = run_lr(df_temp, column=f'percent_change_{demo}')\n",
    "\n",
    "                # Create trend columns\n",
    "                df_temp[f'value_trend_{demo}'] = df_temp['ordinal_date']*coef_value + intercept_value\n",
    "                df_temp[f'percent_change_trend_{demo}'] = df_temp['ordinal_date']*coef_pct + intercept_pct\n",
    "\n",
    "                # Create averages column\n",
    "                the_average_pct = df_temp[f'percent_change_{demo}'].mean()\n",
    "                df_temp[f'average_pct_{demo}'] = the_average_pct\n",
    "                the_average_value = df_temp[f'value_change_{demo}'].mean()\n",
    "                df_temp[f'average_value_{demo}'] = the_average_value\n",
    "                \n",
    "                # Update temp coef\n",
    "                temp_coef_2[f'trend_coef_{demo}'] = coef_value\n",
    "                temp_coef_2[f'average_value_{demo}'] = the_average_value\n",
    "                temp_coef_2[f'average_pct_{demo}'] = the_average_pct\n",
    "            \n",
    "        # Append temp coef to dataframe\n",
    "        coef_df = pd.concat([coef_df, temp_coef_2])\n",
    "            \n",
    "    # Drop duplicates\n",
    "    coef_df = coef_df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Make folder to save coef_df to\n",
    "    trend_folder = \"datasets/cleaned_census_api_files/city_data/city_trend_datasets/\"\n",
    "    create_folder(trend_folder)\n",
    "    \n",
    "    # Create variable string for file naming\n",
    "    var_string = \"all_city_demographic_trends.csv\"\n",
    "        \n",
    "    # Save coef_df\n",
    "    coef_df.to_csv(f\"{trend_folder}/{var_string}\", index=False)\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43442a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make initial trend dataframe with every demographic\n",
    "make_city_trend_dataframes(\n",
    "    df_dict={\n",
    "        \"Jobs\":[jobs, begin_job_year],\n",
    "        \"Population\":[demo_dict['population'], census_begin_year],\n",
    "        \"Median Income\":[demo_dict['median_income'], census_begin_year],\n",
    "        \"Median Price\":[demo_dict['median_price'], census_begin_year],\n",
    "        \"Median Rent\":[demo_dict['median_rent'], census_begin_year],\n",
    "        \"People per Unit\":[demo_dict['people_per_unit'], census_begin_year],\n",
    "        \"Percent Renter Occupied\":[demo_dict['percent_renter_occupied'], census_begin_year],\n",
    "        \"Rent Price Ratio\":[demo_dict['rent_price_ratio'], census_begin_year],\n",
    "        \"Total Employed\":[demo_dict['total_employed'], census_begin_year],\n",
    "    },\n",
    "    rank_city=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba262737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ranking_part_2(\n",
    "    df_dict,\n",
    "    rank_msa=False,\n",
    "    rank_city=False,\n",
    "    list_of_msas=False,\n",
    "    max_price=False,\n",
    "    min_rent_to_price=False,\n",
    "    use_total_trend=True,\n",
    "    use_average_percent=True,\n",
    "    total_trend_weight_dict={},\n",
    "    average_percent_weight_dict={},\n",
    "):\n",
    "    \n",
    "    ### STEP 1: RANK  \n",
    "    \n",
    "    # Define geo_name\n",
    "    if rank_msa:\n",
    "        geo_name = 'msa_name'\n",
    "        geo_file = 'msa'\n",
    "    elif rank_city:\n",
    "        geo_name = 'city_name'\n",
    "        geo_file = 'city'\n",
    "    else:\n",
    "        raise ValueError(\"Please define MSA or City in the arguments.\")\n",
    "        \n",
    "    # Read in the coefficient dataframe\n",
    "    coef_df = pd.read_csv(\"datasets/cleaned_census_api_files/city_data/city_trend_datasets/all_city_demographic_trends.csv\",\n",
    "                         dtype={'city_geoid':str})\n",
    "    \n",
    "    # Filter coef_df by the list of MSAs we want to analyze\n",
    "    if list_of_msas:\n",
    "        coef_df = coef_df[\n",
    "            coef_df['msa_name'].isin(list_of_msas)].reset_index(drop=True)\n",
    "    \n",
    "    # Drop MSAs that have missing values (they will have missing\n",
    "    # values if we couldn't join Census MSAs with BLS MSAs which\n",
    "    # only occurs for a few specific MSAs)\n",
    "    bad_msa = set()\n",
    "    \n",
    "    for demo in df_dict:\n",
    "        \n",
    "        # Filter by nulls\n",
    "        coef_temp = coef_df[coef_df[f'trend_coef_{demo}'].isnull()]\n",
    "        \n",
    "        # Get list of MSAs\n",
    "        bad_msa.update(coef_temp[geo_name].unique())\n",
    "        \n",
    "    # Remove these cities Print helpful message\n",
    "    if len(bad_msa) > 0:\n",
    "        \n",
    "        # Remove cities in bad_msa\n",
    "        coef_df = coef_df[~coef_df[geo_name].isin(bad_msa)].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Removing these MSAs not fit for analysis: {bad_msa}\")\n",
    "        \n",
    "        # Check if the dataframe is now empty\n",
    "        if coef_df.shape[0] == 0:\n",
    "            raise ValueError(\"Dataframe is now empty due to filtering. Please double check.\")\n",
    "        \n",
    "    # Create the rankings for each demographic\n",
    "    for demo in df_dict:\n",
    "        \n",
    "        # Calculate rankings for both, then sort by the total\n",
    "        # ranking. For example, if a city has the highest average\n",
    "        # percent change, it will get a ranking of \"1\" for average_pct,\n",
    "        # and if it has the 8th highest trend coefficient, it will\n",
    "        # get a ranking of \"8\" for trend_coef. When we add those two\n",
    "        # rankings together, the city will have a total ranking\n",
    "        # of \"9\". In this case, the lower the ranking, the better,\n",
    "        # and we will sort total rankings from lowest to highest.\n",
    "        \n",
    "        # Normalize total trend column\n",
    "        coef_df[f'normalized_trend_coef_{demo}'] = normalize_column(\n",
    "            coef_df[f'trend_coef_{demo}'], min_max_standardized=True)\n",
    "        \n",
    "        # Normalize avg pct column\n",
    "        coef_df[f'normalized_average_pct_{demo}'] = normalize_column(\n",
    "            coef_df[f'average_pct_{demo}'], min_max_standardized=True)\n",
    "        \n",
    "        # Check to see if there are weights, and if not,\n",
    "        # set each weight to 1\n",
    "        if demo in total_trend_weight_dict.keys():\n",
    "            trend_weight = total_trend_weight_dict[demo]\n",
    "        else:\n",
    "            trend_weight = 1\n",
    "            \n",
    "        # Check pct weight dict\n",
    "        if demo in average_percent_weight_dict.keys():\n",
    "            pct_weight = average_percent_weight_dict[demo]\n",
    "        else:\n",
    "            pct_weight = 1\n",
    "            \n",
    "        # Re-adjust weights based on whether we are using\n",
    "        # only total trend, only percent, or both. As an example, \n",
    "        # if we aren't using percent, we set the weight to 0, that\n",
    "        # way the percent weight isn't used when totalling the\n",
    "        # demographic's weight.\n",
    "        if use_total_trend == False:\n",
    "            trend_weight = 0\n",
    "        if use_average_percent == False:\n",
    "            pct_weight = 0\n",
    "        \n",
    "        # Create weights\n",
    "        coef_df[f'{demo}_weight'] = (\n",
    "            (coef_df[f'normalized_trend_coef_{demo}'] * trend_weight) \n",
    "            + (coef_df[f'normalized_average_pct_{demo}'] * pct_weight)\n",
    "        )\n",
    "\n",
    "    # Make final total rank column by adding up\n",
    "    # all demo total rankings\n",
    "    coef_df['total_weight'] = 0\n",
    "    for demo in df_dict:\n",
    "        coef_df['total_weight'] += coef_df[f'{demo}_weight']\n",
    "\n",
    "    # Sort by total weight, highest to lowest\n",
    "    final_df = coef_df.sort_values(\n",
    "        'total_weight', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Merge median rent, price, and income to final df\n",
    "    for demo in [\n",
    "        'population',\n",
    "        'median_income',\n",
    "        'median_price',\n",
    "        'median_rent',\n",
    "        'people_per_unit',\n",
    "        'percent_renter_occupied',\n",
    "        'rent_price_ratio',\n",
    "        'total_employed',\n",
    "    ]:\n",
    "                \n",
    "        # Call in demographic dataset\n",
    "        demo_df = pd.read_csv(\n",
    "            f\"datasets/cleaned_census_api_files/{geo_file}_data/{demo}_{geo_file}.csv\",\n",
    "            dtype={'msa_code':str, 'geo_id':str}\n",
    "        )\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        demo_df = prep_census_datasets(demo_df, msa=rank_msa, city=rank_city)\n",
    "\n",
    "        # Get most recent year for median price\n",
    "        recent_year = demo_df['year'].max()\n",
    "\n",
    "        # Filter by recent_year\n",
    "        recent_year_df = demo_df[demo_df['year']==recent_year].copy()        \n",
    "        \n",
    "        # Only keep certain columns\n",
    "        recent_year_df = recent_year_df[['city_geoid','value']]\n",
    "        \n",
    "        # Rename column\n",
    "        recent_year_df.rename(columns={'value':f'{demo}'}, inplace=True)\n",
    "        \n",
    "        # Merge to final_df\n",
    "        final_df = final_df.merge(recent_year_df, how='left', on='city_geoid')\n",
    "        \n",
    "    # Merge jobs\n",
    "    jobs = pd.read_csv('datasets/bls/raw/most_recent_bls_data.csv',\n",
    "                   dtype={'msa_code':str, 'state_code':str})\n",
    "\n",
    "    # Make sure the date column is in datetime format\n",
    "    jobs['date'] = pd.to_datetime(jobs['date'])\n",
    "\n",
    "    # Replace NECTA Division\n",
    "    jobs['msa_name'] = jobs['msa_name'].apply(lambda x: x.replace(\" NECTA Division\",\"\"))\n",
    "    jobs['msa_name'] = jobs['msa_name'].apply(lambda x: x.replace(\" NECTA\",\"\"))\n",
    "\n",
    "    # Get most recent job date\n",
    "    recent_date = jobs['date'].max()\n",
    "\n",
    "    # Filter jobs so it's the most recent date\n",
    "    new_jobs = jobs[jobs['date']==recent_date].copy().reset_index(drop=True)\n",
    "    \n",
    "    # Call in the msa-to-city crosswalk file\n",
    "    crosswalk = pd.read_csv(\"datasets/helper_datasets/msa_to_city_crosswalk.csv\",\n",
    "                           dtype={'msa_geoid':str, 'city_geoid':str})\n",
    "    \n",
    "    # Only keep IDs\n",
    "    crosswalk = crosswalk[['msa_geoid','city_geoid']]\n",
    "    \n",
    "    # Rename jobs' msa column\n",
    "    new_jobs.rename(columns={'msa_code':'msa_geoid'}, inplace=True)\n",
    "    \n",
    "    # Merge crosswalk to job data\n",
    "    new_jobs = new_jobs.merge(crosswalk, \n",
    "                              how='left', \n",
    "                              on='msa_geoid')\n",
    "\n",
    "    # Only keep certain columns\n",
    "    new_jobs = new_jobs[['city_geoid','value']]\n",
    "    \n",
    "    # Rename column\n",
    "    new_jobs.rename(columns={'value':f'jobs_in_msa'}, inplace=True)\n",
    "\n",
    "    # Merge to final_df\n",
    "    final_df = final_df.merge(new_jobs, \n",
    "                              how='left', \n",
    "                              on='city_geoid')\n",
    "    \n",
    "    # If max price, filter it\n",
    "    if max_price:\n",
    "        final_df = final_df[final_df['median_price']<=max_price].reset_index(drop=True)\n",
    "        \n",
    "    # If min rent-price ratio, filter\n",
    "    if min_rent_to_price:\n",
    "        final_df = final_df[final_df['rent_price_ratio']>=min_rent_to_price].reset_index(drop=True)\n",
    "        \n",
    "    ### STEP 2: FILTER\n",
    "    \n",
    "    # Capture the growth ranking\n",
    "    final_df['growth_ranking'] = final_df.index + 1\n",
    "    \n",
    "    # Filter final_df by rent-price-ration and then percent-renter-occupied\n",
    "    final_df = final_df.sort_values(\n",
    "        ['rent_price_ratio','percent_renter_occupied'],\n",
    "        ascending=[False, False]\n",
    "    ).reset_index(drop=True)\n",
    "        \n",
    "    # If list of MSAs, save ranking so we can\n",
    "    # analyze in QGIS later\n",
    "    if list_of_msas:\n",
    "        \n",
    "        # Create folder\n",
    "        folder_save = \"datasets/cleaned_census_api_files/city_data/city_rankings\"\n",
    "        create_folder(folder_save)\n",
    "        \n",
    "        # Get variable name for folder and file\n",
    "        var_string = \"_\".join(list_of_msas)\n",
    "        var_string += \"_rankings.csv\"\n",
    "        \n",
    "        # Save file\n",
    "        final_df.to_csv(f\"{folder_save}/{var_string}\", index=False)\n",
    "    \n",
    "            \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df69c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af603c4e",
   "metadata": {},
   "source": [
    "### Begin Ranking Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standard ranking arguments\n",
    "standard_rank_kwargs = {\n",
    "    'df_dict':{\n",
    "        \"Population\":[demo_dict['population'], census_begin_year],\n",
    "        \"Median Income\":[demo_dict['median_income'], census_begin_year],\n",
    "        \"Median Price\":[demo_dict['median_price'], census_begin_year],\n",
    "        \"Median Rent\":[demo_dict['median_rent'], census_begin_year],\n",
    "    },\n",
    "    'rank_city':True,\n",
    "    'use_total_trend':True,\n",
    "    'use_average_percent':True,\n",
    "    'total_trend_weight_dict':{\n",
    "        \"Population\":0.75,\n",
    "        \"Median Rent\":0.5,\n",
    "        \"Median Income\":0.25,\n",
    "        \"Median Price\":0\n",
    "    },\n",
    "    'average_percent_weight_dict':{\n",
    "        \"Population\":4,\n",
    "        \"Median Rent\":3,\n",
    "        \"Median Income\":2,\n",
    "        \"Median Price\":1\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b6c57f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here is an example of using all arguments \n",
    "# without standard ranking kwargs\n",
    "Greenville_MSA = make_ranking_part_2(\n",
    "    df_dict={\n",
    "        \"Population\":[demo_dict['population'], census_begin_year],\n",
    "        \"Median Income\":[demo_dict['median_income'], census_begin_year],\n",
    "        \"Median Price\":[demo_dict['median_price'], census_begin_year],\n",
    "        \"Median Rent\":[demo_dict['median_rent'], census_begin_year],\n",
    "    },\n",
    "    rank_city=True,\n",
    "    list_of_msas=['Greenville-Anderson, SC'],\n",
    "   use_total_trend=True,\n",
    "    use_average_percent=True,\n",
    "    total_trend_weight_dict={\n",
    "        \"Population\":0.75,\n",
    "        \"Median Rent\":0.5,\n",
    "        \"Median Income\":0.25,\n",
    "        \"Median Price\":0,\n",
    "    },\n",
    "    average_percent_weight_dict={\n",
    "        \"Population\":4,\n",
    "        \"Median Rent\":3,\n",
    "        \"Median Income\":2,\n",
    "        \"Median Price\":1,\n",
    "    }\n",
    ")\n",
    "print(Greenville_MSA.shape)\n",
    "Greenville_MSA.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32c8af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Here is an example of using the standard ranking keyword arguments\n",
    "greenville_test = make_ranking_part_2(\n",
    "    list_of_msas=['Greenville-Anderson, SC'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "greenville_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24bd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make rankings for top MSAs\n",
    "Dallas_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Dallas-Fort Worth-Arlington, TX'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Charleston_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Charleston-North Charleston, SC'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Charlotte_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Charlotte-Concord-Gastonia, NC-SC'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Athens_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Athens-Clarke County, GA'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Nashville_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Nashville-Davidson--Murfreesboro--Franklin, TN'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Atlanta_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Atlanta-Sandy Springs-Alpharetta, GA'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Austin_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Austin-Round Rock-Georgetown, TX'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Raleigh_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Raleigh-Cary, NC'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Orlando_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Orlando-Kissimmee-Sanford, FL'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Tampa_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Tampa-St. Petersburg-Clearwater, FL'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Knoxville_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Knoxville, TN'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Sherman_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Sherman-Denison, TX'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Birmingham_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Birmingham-Hoover, AL'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Los_Angeles_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Los Angeles-Long Beach-Anaheim, CA'],\n",
    "    **standard_rank_kwargs\n",
    ")\n",
    "\n",
    "Phoenix_MSA = make_ranking_part_2(\n",
    "    list_of_msas=['Phoenix-Mesa-Chandler, AZ'],\n",
    "    **standard_rank_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9031ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427768e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3292cb25",
   "metadata": {},
   "source": [
    "### Define Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac95848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through all cities, sort by coefficient, plot top 10\n",
    "def plot_msa_cities(\n",
    "    cities_in_MSA,\n",
    "    just_city=True,\n",
    "    plot_population=False,\n",
    "    plot_rent=False,\n",
    "    plot_income=False,\n",
    "    plot_price=False,\n",
    "    plot_units=False,\n",
    "    plot_rent_to_price=False,\n",
    "    plot_people_per_unit=False,\n",
    "    plot_percent_renter_occupied=False,\n",
    "    plot_total_employed=False,\n",
    "    begin_year_1=2013,\n",
    "    plot_all=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the top cities for a given demographic. Top cities\n",
    "    are chosen based on their trend. This function can also \n",
    "    find the top cities based on multiple datasets.\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "        ranked_cities (DataFrame): A dataframe of cities\n",
    "            already ranked by various demographics. The\n",
    "            dataframe returned by the \"make_ranking()\"\n",
    "            function is the ideal dataframe to pass\n",
    "            to this function.\n",
    "            \n",
    "        plot_jobs (True/False): If Ture, plot jobs. Only one \n",
    "            demographic can be plotted at a time, so if you'd \n",
    "            like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "        \n",
    "        plot_rent (True/False): If Ture, plot rent. Only one \n",
    "            demographic can be plotted at a time, so if you'd \n",
    "            like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "        \n",
    "        plot_income (True/False): If Ture, plot income. Only one \n",
    "            demographic can be plotted at a time, so if you'd \n",
    "            like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "        \n",
    "        plot_price (True/False): If Ture, plot price. Only one \n",
    "            demographic can be plotted at a time, so if you'd \n",
    "            like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "        \n",
    "        plot_units (True/False): If Ture, plot units. Only one \n",
    "            demographic can be plotted at a time, so if you'd \n",
    "            like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "        \n",
    "        plot_rent_to_price (True/False): If Ture, plot rent-ro-price\n",
    "            ratio. Only one demographic can be plotted at a time, \n",
    "            so if you'd like to plot a different demographic, \n",
    "            this must be set to False.\n",
    "        \n",
    "        plot_jobs_per_unit (True/False): If Ture, plot jobs-per-unit. \n",
    "            Only one demographic can be plotted at a time, so if \n",
    "            you'd like to plot a different demographic, this must \n",
    "            be set to False.\n",
    "            \n",
    "        begin_year_1 (int): The year you'd like the\n",
    "            analysis to start.\n",
    "            \n",
    "        plot_all (True/False): True if you want to plot every\n",
    "            city in the dataframe (slow). False if you only want\n",
    "            to plot the top 5 (fast).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "        coef_df (DataFrame): A dataframe with the rankings\n",
    "            of each city, from \"best to worst.\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Make a copy of the ranked cities\n",
    "    ranked = cities_in_MSA.copy()\n",
    "    \n",
    "    # Create folders to save graphs into\n",
    "    create_folder(\"graphs\")\n",
    "    create_folder(\"graphs/city_graphs_by_MSA\")\n",
    "    \n",
    "    ### Call in the dataset we will be graphing from\n",
    "    \n",
    "    # Population \n",
    "    if plot_population:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Population\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            \"datasets/cleaned_census_api_files/city_data/population_city.csv\",\n",
    "            dtype={'geo_id':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, city=just_city)\n",
    "    \n",
    "    # If plotting rent growth\n",
    "    elif plot_rent:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Median Rent\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            \"datasets/cleaned_census_api_files/city_data/median_rent_city.csv\",\n",
    "            dtype={'geo_id':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, city=just_city)\n",
    "        \n",
    "    # If plotting income growth\n",
    "    elif plot_income:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Median Income\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            \"datasets/cleaned_census_api_files/city_data/median_income_city.csv\",\n",
    "            dtype={'geo_id':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, city=just_city)\n",
    "        \n",
    "    # If plotting price growth\n",
    "    elif plot_price:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Median Price\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            \"datasets/cleaned_census_api_files/city_data/median_price_city.csv\",\n",
    "            dtype={'geo_id':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, city=just_city)\n",
    "        \n",
    "    # If plotting unit growth\n",
    "    elif plot_units:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Total Units\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            \"datasets/cleaned_census_api_files/city_data/total_units_city.csv\",\n",
    "            dtype={'geo_id':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, city=just_city)\n",
    "        \n",
    "    # If plotting rent-to-price\n",
    "    elif plot_rent_to_price:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Rent-to-Price\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            \"datasets/cleaned_census_api_files/city_data/rent_price_ratio_city.csv\",\n",
    "            dtype={'geo_id':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, city=just_city)\n",
    "        \n",
    "    elif plot_people_per_unit:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"People-per-Unit\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            \"datasets/cleaned_census_api_files/city_data/people_per_unit_city.csv\",\n",
    "            dtype={'geo_id':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, city=just_city)\n",
    "        \n",
    "    elif plot_percent_renter_occupied:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Percent Renter Occupied\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            \"datasets/cleaned_census_api_files/city_data/percent_renter_occupied_city.csv\",\n",
    "            dtype={'geo_id':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, city=just_city)\n",
    "        \n",
    "    elif plot_total_employed:\n",
    "        \n",
    "        # Set demographic title for graphs\n",
    "        demographic_1=\"Total Employed\"\n",
    "        \n",
    "        # Read in data\n",
    "        dataframe_1 = pd.read_csv(\n",
    "            \"datasets/cleaned_census_api_files/city_data/total_employed_city.csv\",\n",
    "            dtype={'geo_id':str})\n",
    "\n",
    "        # Run prep function to get into correct format\n",
    "        dataframe_1 = prep_census_datasets(dataframe_1, city=just_city)\n",
    "\n",
    "    # Otherwise, print error statement\n",
    "    else:\n",
    "        print(\"Please specify a demographic to plot by setting it to True, and leaving the others set to False.\")\n",
    "        raise Exception(\"SPECIFY A DEMOGRAPHIC TO PLOT.\")\n",
    "    \n",
    "    # Make copy\n",
    "    main_df = dataframe_1.copy()\n",
    "            \n",
    "    # Create main variable to use for the rest of the script\n",
    "    column = 'value'\n",
    "\n",
    "    # Create dictionary to store filtered dataframes\n",
    "    filtered_dict = {}\n",
    "\n",
    "    # Set y_lim list to find max and min\n",
    "    y_lim_list_trend = []\n",
    "    y_min_list_trend = [0]\n",
    "    y_lim_list_pct = []\n",
    "    y_min_list_pct = [0]\n",
    "    \n",
    "    # Loop through all cities in the ranked dataframe\n",
    "    for city in ranked['city_name'].dropna().unique():\n",
    "\n",
    "        # Isolate just that city\n",
    "        df = main_df[main_df['city_name']==city].copy()\n",
    "        \n",
    "        # Get MSA name\n",
    "        msa_name = df['msa_name'].iloc[0]\n",
    "\n",
    "        # Create folders to save graphs into\n",
    "        create_folder(f\"graphs/city_graphs_by_MSA/{msa_name}\")\n",
    "        \n",
    "        # Sort by date\n",
    "        df = df.sort_values('date')\n",
    "                \n",
    "        # Create difference column\n",
    "        df['value_change'] = df['value'].diff()\n",
    "        \n",
    "        # Create pct_change column\n",
    "        df['percent_change'] = df['value'].pct_change()\n",
    "        \n",
    "        # Filter by beginning year\n",
    "        df = df[df['year']>=begin_year_1].reset_index(drop=True)\n",
    "                \n",
    "        # If an MSA's most recent year is after the beginning\n",
    "        # year, remove it from the graphs. For example, if we want to\n",
    "        # view the growth of all cities since 2016, but Prescott Valley\n",
    "        # only has data starting at 2019, this may skew the data.\n",
    "        if df['year'].iloc[0] != begin_year_1:\n",
    "            print(f\"Dropping {city}, it's dataframe has a smaller window.\")\n",
    "            continue\n",
    "                \n",
    "        # Remove NaN values\n",
    "        df = df[df['percent_change'].notna()]\n",
    "        \n",
    "        # Isolate date and value columns\n",
    "        df = df[[\n",
    "            'msa_name',\n",
    "            'date', 'value', 'value_change',\n",
    "            'percent_change']].reset_index(drop=True)\n",
    "        \n",
    "        # Add this dataframe's y_lim to list\n",
    "        y_lim_list_trend.append(df['value'].max())\n",
    "        y_min_list_trend.append(df['value'].min())\n",
    "        y_lim_list_pct.append(df['percent_change'].max())\n",
    "        y_min_list_pct.append(df['percent_change'].min())\n",
    "        \n",
    "        # get next months's datetime\n",
    "        next_year = df['date'].iloc[-1] + relativedelta(months=1)\n",
    "        \n",
    "        # Create ordinal column\n",
    "        df['ordinal_date'] = df['date'].map(datetime.toordinal)\n",
    "        \n",
    "        # Run linear regression and get the trend's coefficient\n",
    "        coef_value, intercept_value = run_lr(df, column='value')\n",
    "        coef_pct, intercept_pct = run_lr(df, column='percent_change')\n",
    "        \n",
    "        # Create next year's date\n",
    "        df.loc[len(df.index)] = [\n",
    "            msa_name,\n",
    "            next_year, np.nan, np.nan, \n",
    "            np.nan, datetime.toordinal(next_year)]\n",
    "        \n",
    "        # Create averages column\n",
    "        the_average_pct = df['percent_change'].mean()\n",
    "        df['average_pct'] = the_average_pct\n",
    "        the_average_value = df['value_change'].mean()\n",
    "        df['average_value'] = the_average_value\n",
    "\n",
    "        # Fill in with linear regression values.\n",
    "        # Also add highest trend value to lim_list.\n",
    "        df['value_trend'] = df['ordinal_date']*coef_value + intercept_value\n",
    "        df['percent_change_trend'] = df['ordinal_date']*coef_pct + intercept_pct\n",
    "\n",
    "        # Also add highest trend value to lim_list\n",
    "        y_lim_list_trend.append(df['value_trend'].max())\n",
    "        y_lim_list_pct.append(df['percent_change_trend'].max())\n",
    "\n",
    "        # Get the y_lim\n",
    "        y_lim_trend = max(y_lim_list_trend) * 1.1\n",
    "        y_min_trend = min(y_min_list_trend)\n",
    "        y_lim_pct = max(y_lim_list_pct) * 1.1\n",
    "        y_min_pct = min(y_min_list_pct)\n",
    "            \n",
    "        # Save filtered data to dictionary\n",
    "        filtered_dict[city] = df\n",
    "    \n",
    "    # Loop through each city in the ranked df\n",
    "    for city_name in ranked['city_name']:\n",
    "\n",
    "        # Get the job data\n",
    "        df = filtered_dict[city_name].copy()\n",
    "        \n",
    "        # Get msa_name\n",
    "        msa_name = df['msa_name'].iloc[0]\n",
    "            \n",
    "        # Make a grid to plot 2 graphs on\n",
    "        fig = plt.figure(figsize=(12,3), dpi=300)\n",
    "        gs = GridSpec(nrows=1, ncols=2)\n",
    "        ax1 = fig.add_subplot(gs[0,0])\n",
    "        ax2 = fig.add_subplot(gs[0,1])\n",
    "        ax_list = [ax1, ax2]\n",
    "        \n",
    "        # Set grid\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "        # Set title\n",
    "        fig.suptitle(f\"{city_name}\\n\\n\\n\", \n",
    "             fontweight=\"bold\")\n",
    "\n",
    "        # Plot first graph\n",
    "        ax1 = df.plot(x='date',y='value', ax=ax1)\n",
    "        ax1 = df.plot(x='date',y='value_trend', ax=ax1, linestyle=\"--\")\n",
    "\n",
    "        # Plot second graph\n",
    "        ax2 = df.plot(x='date',y='percent_change', ax=ax2)\n",
    "        ax2 = df.plot(x='date',y='percent_change_trend', ax=ax2, linestyle=\"--\")\n",
    "\n",
    "        # Second graph's zero line\n",
    "        df['zero'] = 0\n",
    "        ax2 = df.plot(x='date', y='zero', ax=ax2, color=\"grey\")\n",
    "\n",
    "        # Also plot the average line\n",
    "        ax2 = df.plot(x='date', y='average_pct', \n",
    "                      ax=ax2, color=\"black\", linestyle=\"-\")\n",
    "\n",
    "        # Set title's for both graphs\n",
    "        ax1.set_title(f\"{demographic_1} Growth\")\n",
    "        ax2.set_title(f\"Percent Change in {demographic_1}\")\n",
    "\n",
    "        # Set y lims and y ticks\n",
    "        ax1.set_ylim([y_min_trend, y_lim_trend])\n",
    "        ax2.set_ylim([y_min_pct, y_lim_pct])\n",
    "\n",
    "        # Set y limits\n",
    "        y_tick_list_trend = [\n",
    "            y_lim_trend*0.25, y_lim_trend*0.5, \n",
    "            y_lim_trend*0.75, y_lim_trend]\n",
    "        y_tick_list_pct = [\n",
    "            y_min_pct, y_min_pct*0.5, 0, \n",
    "            y_lim_pct*0.5, y_lim_pct]\n",
    "\n",
    "        # Set y_ticks\n",
    "        ax1.yaxis.set_major_locator(\n",
    "            mticker.FixedLocator(y_tick_list_trend))\n",
    "        ax2.yaxis.set_major_locator(\n",
    "            mticker.FixedLocator(y_tick_list_pct))\n",
    "\n",
    "        # Set y-tick labels\n",
    "        ax1.set_yticklabels(\n",
    "            ['{:,}'.format(round(float(x), 3)) for x in y_tick_list_trend])\n",
    "        ax2.set_yticklabels(\n",
    "            ['{:,}'.format(round(float(x), 3)) for x in y_tick_list_pct])\n",
    "        \n",
    "        # Give suptitle more room\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "\n",
    "        # Create folder to save graphs into\n",
    "        create_folder(f\"graphs/city_graphs_by_MSA/{msa_name}/{city_name}\")\n",
    "\n",
    "        # Create filepath to save graph to\n",
    "        save_filepath = f\"graphs/city_graphs_by_MSA/{msa_name}/{city_name}/{demographic_1} Growth.png\"\n",
    "\n",
    "        # Save the graphs\n",
    "        plt.savefig(save_filepath)\n",
    "        \n",
    "        # Show plot\n",
    "        if not plot_all:\n",
    "            plt.show()\n",
    "        \n",
    "        # Clear plot\n",
    "        plt.close(\"all\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0229c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot every demographic\n",
    "def plot_every_demographic_cities(\n",
    "    ranking_df, \n",
    "    begin_job_year=begin_job_year, \n",
    "    census_begin_year=census_begin_year,\n",
    "    plot_all=False\n",
    "):\n",
    "    \n",
    "    # Plot every demographic \n",
    "    demo_kwargs = {\n",
    "        'plot_population':False,\n",
    "        'plot_rent':False,\n",
    "        'plot_income':False,\n",
    "        'plot_price':False,\n",
    "        'plot_units':False,\n",
    "        'plot_rent_to_price':False,\n",
    "        'plot_people_per_unit':False,\n",
    "        'plot_percent_renter_occupied':False,\n",
    "        'plot_total_employed':False,\n",
    "    }\n",
    "\n",
    "    # Loop through each demo argument\n",
    "    for demo_arg in demo_kwargs:\n",
    "\n",
    "        # Make a copy\n",
    "        demo_kwargs_copy = demo_kwargs.copy()\n",
    "\n",
    "        # Make the demo set to True\n",
    "        demo_kwargs_copy[demo_arg] = True\n",
    "\n",
    "        # Plot\n",
    "        plot_msa_cities(\n",
    "            cities_in_MSA=ranking_df,\n",
    "            begin_year_1=census_begin_year,\n",
    "            plot_all=plot_all,\n",
    "            **demo_kwargs_copy\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde93a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here to test Greenville\n",
    "plot_msa_cities(\n",
    "    cities_in_MSA=Greenville_MSA,\n",
    "    plot_population=True,\n",
    "    plot_rent=False,\n",
    "    plot_income=False,\n",
    "    plot_price=False,\n",
    "    plot_units=False,\n",
    "    plot_rent_to_price=False,\n",
    "    plot_people_per_unit=False,\n",
    "    plot_percent_renter_occupied=False,\n",
    "    plot_total_employed=False,\n",
    "    begin_year_1=2016,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5552e56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6c436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a0d1193",
   "metadata": {},
   "source": [
    "### Plot Actual Maps of Cities in MSA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228e049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_msa_cities_map(\n",
    "    cities_in_MSA,\n",
    "    using_list_of_MSAs=False\n",
    "):\n",
    "    \n",
    "    # Make copy\n",
    "    if not using_list_of_MSAs:\n",
    "        df = cities_in_MSA.copy()\n",
    "        \n",
    "    # If there is a list of MSAs close to each\n",
    "    # other, we can map them. Just concat the\n",
    "    # dataframes.\n",
    "    elif using_list_of_MSAs:\n",
    "        \n",
    "        # Make new empty df\n",
    "        df = pd.DataFrame(data=None, columns=cities_in_MSA[0].columns)\n",
    "        \n",
    "        # Loop through the list and concat\n",
    "        for dataframe in cities_in_MSA:\n",
    "            \n",
    "            new_df = dataframe.copy()\n",
    "            df = pd.concat([df, new_df])\n",
    "            \n",
    "        # Reset index\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    \n",
    "    # Call in geometry file\n",
    "    city_geom = gp.read_file(\n",
    "        \"datasets/census_original_files/census_geopackages/city_geometries.gpkg\",\n",
    "        dtype={'GEOID':str})\n",
    "    \n",
    "    # Rename city geoid column\n",
    "    city_geom.rename(columns={'GEOID':'city_geoid'}, inplace=True)\n",
    "    \n",
    "    # Get number of rows of df\n",
    "    num_rows = df.shape[0]\n",
    "    \n",
    "    # Merge geometries\n",
    "    new_df = df.merge(city_geom, how=\"inner\", on=\"city_geoid\")\n",
    "    \n",
    "    # Get number of rows of new dataframe\n",
    "    new_num_rows = new_df.shape[0]\n",
    "    \n",
    "    # Print statement if cities were lost in the merge\n",
    "    if num_rows != new_num_rows:\n",
    "        \n",
    "        # Get set of original cities\n",
    "        og_cities = set(df['city_name'])\n",
    "        \n",
    "        # Get set of merged cities\n",
    "        merged_cities = set(new_df['city_geoid'])\n",
    "        \n",
    "        # Get cities left out\n",
    "        cities_left_out = og_cities - merged_cities\n",
    "        \n",
    "        # Print cities left out\n",
    "        print(f'The following cities were not included in the merge: {cities_left_out}')\n",
    "    \n",
    "    # Ensure the new_df is a geodataframe\n",
    "    new_df = gp.GeoDataFrame(new_df)\n",
    "    \n",
    "    # Enforce a standard crs\n",
    "    new_df = new_df.to_crs(epsg=3857)\n",
    "    \n",
    "    # get MSA name\n",
    "    msa_name = new_df['msa_name'].iloc[0]\n",
    "\n",
    "    # Create folder to save graphs into\n",
    "    create_folder(f\"graphs/city_graphs_by_MSA\")\n",
    "    create_folder(f\"graphs/city_graphs_by_MSA/{msa_name}\")\n",
    "    create_folder(f\"graphs/city_graphs_by_MSA/{msa_name}/maps\")\n",
    "\n",
    "    def create_map(\n",
    "        map_name=\"Basemap\",\n",
    "        plot_price=False,\n",
    "        plot_population=False,\n",
    "        plot_rent=False,\n",
    "        plot_income=False,\n",
    "        plot_rent_to_price=False,\n",
    "        plot_people_per_unit=False,\n",
    "        plot_percent_renter_occupied=False,\n",
    "        plot_total_employed=False,\n",
    "        plot_avg_percent_population=False,\n",
    "        plot_avg_percent_median_rent=False,\n",
    "    ):\n",
    "        \n",
    "        # Adjust the figure's figsize based on\n",
    "        # how many cities there are\n",
    "        if new_df.shape[0] <= 10:\n",
    "            fig_xy = 10\n",
    "        elif new_df.shape[0] <= 20:\n",
    "            fig_xy = 15\n",
    "        elif new_df.shape[0] <= 30:\n",
    "            fig_xy = 20\n",
    "        elif new_df.shape[0] <= 40:\n",
    "            fig_xy = 25\n",
    "        else:\n",
    "            fig_xy = 30\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(fig_xy,fig_xy))\n",
    "\n",
    "        # Plot the towns\n",
    "        new_df.boundary.plot(ax=ax)\n",
    "\n",
    "        # Plot basemap\n",
    "        cx.add_basemap(ax, alpha=0.4)\n",
    "\n",
    "        # Place labels inside the geometries. More options can\n",
    "        # be found in the documentation here:\n",
    "        # https://matplotlib.org/stable/tutorials/text/annotations.html\n",
    "        new_df.apply(\n",
    "            lambda x: ax.annotate(text=x['NAME'], \n",
    "            xy=x.geometry.centroid.coords[0], \n",
    "            ha='center', size=8, bbox=dict(\n",
    "                boxstyle=\"round,pad=0.3\", fc=\"white\", \n",
    "                ec=\"black\", lw=1)), axis=1)\n",
    "\n",
    "        # Establish add_plot variable to only change\n",
    "        # if we aren't plotting a basemap\n",
    "        add_plot = False\n",
    "        adjust_format = False\n",
    "\n",
    "        # If plotting, add details into map\n",
    "        if plot_price:\n",
    "            the_column=\"median_price\"\n",
    "            cmap=\"Reds\"\n",
    "            add_plot=True\n",
    "            map_name=\"Median Price\"\n",
    "        elif plot_population:\n",
    "            the_column=\"population\"\n",
    "            cmap=\"Blues\"\n",
    "            add_plot=True\n",
    "            map_name=\"Population\"\n",
    "        elif plot_rent:\n",
    "            the_column=\"median_rent\"\n",
    "            cmap=\"Greens\"\n",
    "            add_plot=True\n",
    "            map_name=\"Median Rent\"\n",
    "        elif plot_income:\n",
    "            the_column=\"median_income\"\n",
    "            cmap=\"Reds\"\n",
    "            add_plot=True\n",
    "            map_name=\"Median Income\"\n",
    "        elif plot_rent_to_price:\n",
    "            the_column=\"rent_price_ratio\"\n",
    "            cmap=\"Blues\"\n",
    "            add_plot=True\n",
    "            map_name=\"Rent-to-Price Ratio\"\n",
    "            adjust_format=True\n",
    "        elif plot_people_per_unit:\n",
    "            the_column=\"people_per_unit\"\n",
    "            cmap=\"Greens\"\n",
    "            add_plot=True\n",
    "            map_name=\"People-per-Unit\"\n",
    "        elif plot_percent_renter_occupied:\n",
    "            the_column=\"percent_renter_occupied\"\n",
    "            cmap=\"Reds\"\n",
    "            add_plot=True\n",
    "            map_name=\"Percent Renter-Occupied\"\n",
    "            adjust_format=True\n",
    "        elif plot_total_employed:\n",
    "            the_column=\"total_employed\"\n",
    "            cmap=\"Blues\"\n",
    "            add_plot=True\n",
    "            map_name=\"Total Employed\"\n",
    "        elif plot_avg_percent_population:\n",
    "            the_column=\"average_pct_Population\"\n",
    "            cmap=\"Reds\"\n",
    "            add_plot=True\n",
    "            map_name=\"Avg. Population Growth\"\n",
    "            adjust_format=True\n",
    "        elif plot_avg_percent_median_rent:\n",
    "            the_column=\"average_pct_Median Rent\"\n",
    "            cmap=\"Greens\"\n",
    "            add_plot=True\n",
    "            map_name=\"Avg. Rent Growth\"\n",
    "            adjust_format=True\n",
    "\n",
    "        # Establish legend keywords if adding plot\n",
    "        if add_plot:\n",
    "            legend_kwds={\n",
    "                    'frameon':True,\n",
    "                    'title':f'{map_name}',\n",
    "                    'loc': 'center left', \n",
    "                    'bbox_to_anchor':(1,0.5)}\n",
    "\n",
    "            # Further adjust for rent-price-ratio\n",
    "            if adjust_format:\n",
    "                legend_kwds['fmt'] = '{:.2%}'\n",
    "\n",
    "\n",
    "        # Add new plot if not basemap!\n",
    "        if add_plot:\n",
    "\n",
    "            new_df.plot(\n",
    "                column=the_column, ax=ax, \n",
    "                legend=True, cmap=cmap, scheme=\"quantiles\",\n",
    "                legend_kwds=legend_kwds)\n",
    "\n",
    "        # Set title\n",
    "        plt.title(f\"{msa_name} MSA - {map_name}\", fontweight=\"bold\")\n",
    "\n",
    "        # Congifure axes\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        # Create filepath to save basemap\n",
    "        save_filepath = f\"graphs/city_graphs_by_MSA/{msa_name}/maps/{map_name}_{msa_name}.png\"\n",
    "\n",
    "        # Save basemap\n",
    "        plt.savefig(save_filepath, bbox_inches=\"tight\")\n",
    "\n",
    "        # Show plot, then clear\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "    # Create all maps\n",
    "    create_map()\n",
    "    create_map(plot_price=True)\n",
    "    create_map(plot_population=True)\n",
    "    create_map(plot_rent=True)\n",
    "    create_map(plot_income=True)\n",
    "    create_map(plot_rent_to_price=True)\n",
    "    create_map(plot_people_per_unit=True)\n",
    "    create_map(plot_percent_renter_occupied=True)\n",
    "    create_map(plot_total_employed=True)\n",
    "    create_map(plot_avg_percent_population=True)\n",
    "    create_map(plot_avg_percent_median_rent=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of top MSAs\n",
    "top_msa_list = [\n",
    "    Greenville_MSA, Dallas_MSA, Charleston_MSA, Charlotte_MSA,\n",
    "    Athens_MSA, Nashville_MSA, Atlanta_MSA, Austin_MSA,\n",
    "    Raleigh_MSA, Orlando_MSA, Tampa_MSA, Knoxville_MSA,\n",
    "    Sherman_MSA, Birmingham_MSA, Los_Angeles_MSA, Phoenix_MSA\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb310c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Make a dataframe with every city in our list\n",
    "all_cities_in_top_msas = pd.concat(top_msa_list).copy().reset_index(drop=True)\n",
    "all_cities_in_top_msas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8316e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Georgia's MSAs\n",
    "Georgia_MSA = plot_msa_cities_map(\n",
    "    [Atlanta_MSA, Athens_MSA],\n",
    "    using_list_of_MSAs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d45e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Double MSA\n",
    "Georgia_MSA[\n",
    "    (Georgia_MSA['percent_renter_occupied']>0.3)\n",
    "    & (Georgia_MSA['median_price']<250000)\n",
    "].sort_values('average_pct_Median Income', ascending=False)\n",
    "\n",
    "\n",
    "# Georgia_MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503692c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Dallas with Sherman-Denison\n",
    "North_Texas_MSA = plot_msa_cities_map(\n",
    "    [Dallas_MSA, Sherman_MSA],\n",
    "    using_list_of_MSAs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addbe029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Double MSA\n",
    "North_Texas_MSA[\n",
    "    (North_Texas_MSA['percent_renter_occupied']>0.3)\n",
    "    & (North_Texas_MSA['median_price']<250000)\n",
    "].sort_values('rent_price_ratio', ascending=False)\n",
    "\n",
    "\n",
    "# North_Texas_MSA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae0a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot maps of our top MSAs\n",
    "for msa in top_msa_list:\n",
    "    plot_msa_cities_map(msa)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a991b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Charleston_MSA[\n",
    "    (Charleston_MSA['percent_renter_occupied']>0.3)\n",
    "    & (Charleston_MSA['median_price']<250000)\n",
    "].sort_values('average_pct_Median Income', ascending=False)\n",
    "\n",
    "Charleston_MSA.sort_values('average_pct_Median Rent', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7343050",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities_in_top_msas[all_cities_in_top_msas['city_name'].str.contains(\"San Marcos\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeea5de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Isolate specific cities to comapre against each other\n",
    "specific_city_list = \"\"\"McDonough|Fairburn|Villa Rica|North Charleston|Summerville|Sherman|Denison|San Marcos\"\"\"\n",
    "\n",
    "top_cities = all_cities_in_top_msas[\n",
    "    all_cities_in_top_msas['city_name'].str.contains(specific_city_list)\n",
    "].copy().reset_index(drop=True)\n",
    "\n",
    "top_cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573e297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot demographics for specific cities\n",
    "plot_every_demographic_cities(top_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012e3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Run stats on top_cities\n",
    "top_cities.sort_values('rent_price_ratio', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30821232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c42fbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "housing_supply_and_demand",
   "language": "python",
   "name": "housing_supply_and_demand"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
